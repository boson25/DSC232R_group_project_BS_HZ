{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e16ea08-75d7-4b2e-a08b-d501d0ce8f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- processid: string (nullable = true)\n",
      " |-- sampleid: string (nullable = true)\n",
      " |-- taxon: string (nullable = true)\n",
      " |-- phylum: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- order: string (nullable = true)\n",
      " |-- family: string (nullable = true)\n",
      " |-- dna_bin: string (nullable = true)\n",
      " |-- dna_barcode: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- province_state: string (nullable = true)\n",
      " |-- coord-lat: float (nullable = true)\n",
      " |-- coord-lon: float (nullable = true)\n",
      " |-- image_measurement_value: double (nullable = true)\n",
      " |-- area_fraction: double (nullable = true)\n",
      " |-- scale_factor: double (nullable = true)\n",
      " |-- inferred_ranks: integer (nullable = true)\n",
      " |-- split: string (nullable = true)\n",
      " |-- k_mer: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- one_hot_encoded: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- sequence_length: integer (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# upload data and print schema\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Clean Metadata').getOrCreate()\n",
    "\n",
    "df_structured_raw = spark.read.parquet(\"/expanse/lustre/projects/uci150/hzhao16/clean_metadata_v2.parquet\")\n",
    "\n",
    "df_structured_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6514fae0-53de-4043-98e8-5be73be3b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select several columns to make predictions on 'order'\n",
    "selected_columns = [\n",
    "    'coord-lat', 'coord-lon', \n",
    "    'image_measurement_value', \n",
    "    'area_fraction', 'scale_factor',\n",
    "    'sequence_length', 'inferred_ranks',\n",
    "    'order'\n",
    "]\n",
    "\n",
    "# drop missing values\n",
    "df_structured_model =df_structured_raw.select(*selected_columns).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a03755-628a-4c1c-a9ee-9e93233dcb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "label_indexer = StringIndexer(inputCol= 'order', outputCol = 'label')\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assmeber = VectorAssembler(\n",
    "    inputCols = [\n",
    "    'coord-lat', 'coord-lon', \n",
    "    'image_measurement_value', \n",
    "    'area_fraction', 'scale_factor', \n",
    "    'sequence_length', 'inferred_ranks',\n",
    "],\n",
    "    outputCol = 'features'\n",
    ")\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='label', featuresCol='features', numTrees=15, maxDepth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952bdbaa-a04c-47ef-92cd-edf5dd1c5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [label_indexer, assmeber, rf])\n",
    "\n",
    "# split data into training data and test data\n",
    "df_structured_train, df_structured_test = df_structured_model.randomSplit([0.8, 0.2], seed = 42)\n",
    "model_structured_rf = pipeline.fit(df_structured_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062e8ed9-391a-4089-ae28-2371e1e08302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_structured_pred = model_structured_rf.transform(df_structured_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff265ae0-8eb4-4ef7-b74b-69229b3ec3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7556\n",
      "Macro F1 Score: 0.6777\n"
     ]
    }
   ],
   "source": [
    "# make predictions on test data\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol='label', predictionCol='prediction', metricName='accuracy'\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol='label', predictionCol='prediction', metricName='f1'\n",
    ")\n",
    "\n",
    "# evaluate test accuracy and macro-F1\n",
    "accuracy = evaluator_acc.evaluate(df_structured_pred)\n",
    "f1 = evaluator_f1.evaluate(df_structured_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Macro F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef3e381-a0ef-4eb5-9fe1-0ae6378a64f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7552\n",
      "Training Macro F1 Score: 0.6772\n"
     ]
    }
   ],
   "source": [
    "# make predictions on training data\n",
    "evaluator_acc_train = MulticlassClassificationEvaluator(\n",
    "    labelCol='label', predictionCol='prediction', metricName='accuracy'\n",
    ")\n",
    "\n",
    "evaluator_f1_train = MulticlassClassificationEvaluator(\n",
    "    labelCol='label', predictionCol='prediction', metricName='f1'\n",
    ")\n",
    "\n",
    "# evaluate traing accuracy and macro-F1\n",
    "train_predictions = model_structured_rf.transform(df_structured_train)\n",
    "\n",
    "train_accuracy = evaluator_acc_train.evaluate(train_predictions)\n",
    "train_f1 = evaluator_f1_train.evaluate(train_predictions)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "print(f'Training Macro F1 Score: {train_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb1720-cd4a-48ff-bd18-29ed12077183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Graph Analysis:\n",
    "# The training accuracy (0.7552) and test accuracy (0.7556) are very close,\n",
    "# and the macro F1 scores are also similar. This indicates that the model is\n",
    "# neither underfitting nor overfitting â€” it fits the data well.\n",
    "\n",
    "# Our model is currently in the \"Good Fit\" zone on the fitting graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82fa19-8c92-4124-8c20-eb438bb77f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
